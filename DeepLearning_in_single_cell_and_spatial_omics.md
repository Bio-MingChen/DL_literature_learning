<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# 文献阅读记录：Deep Learning in Single-Cell and Spatial Transcriptomics Data Analysis: Advances and Challenges from a Data Science Perspective


## 维度灾难(Curse of Dimensionality)

维度灾难（Curse of Dimensionality）指的是随着维度数量的增加，空间的体积呈指数级增长，导致数据在空间中变得稀疏，使得有限的观测数据难以有效覆盖整个空间。因此，数据点之间的相似性会降低。为了解决这个问题，通常采用特征选择（Feature Selection）和降维（Dimensionality Reduction）技术。

传统的降维方法包括：
- **参数化方法（Parametric Methods）**：如**主成分分析（PCA）**及其变体，例如 **scPCA** 和 **FastRNA**。
- **非参数化方法（Non-Parametric Methods）**：如 **t-分布随机邻域嵌入（t-SNE）** 和 **统一流形逼近与投影（UMAP）**。这些方法的目标是在映射数据到低维空间的同时，尽可能保留局部结构，通常使用概率或度量学习的方式来定义数据点之间的关系。

相比之下，参数化方法通过明确的数学模型建立数据点之间的关系，并从训练数据中估计参数。深度学习（Deep Learning, DL）是一种强大的参数化方法，利用神经网络进行自动建模，能够直接从数据中进行端到端的参数学习。与传统方法相比，深度学习在处理复杂和高维特征空间方面表现出更强的效果。

**Scvis** 是一种基于变分自编码器（Variational Autoencoder, VAE）框架的非线性降维方法，它结合了生成模型（Generative Modeling）和变分推断（Variational Inference），通过两个不同的神经网络结构，实现高维数据与低维空间（即**细胞嵌入**）之间的双向映射，从而保留数据的全局结构。

假设一个高维的单细胞 RNA 测序（scRNA-seq）数据集 \(D = \{x_n\}_{n=1}^{N}\)，包含 \(N\) 个细胞，每个细胞的基因表达表示为 \(x_n\)。通常假设观测数据是由低维先验分布生成的，其概率模型如下：
\[
p(x_n | z_n, \theta) = \text{Distribution}(\mu_{\theta}(z_n), \sigma_{\theta}(z_n))
\]
其中，**低维潜变量** \(z_n\) 服从标准正态分布：
\[
p(z_n | \theta) = \prod_{i=1}^{d}N(z_{n,i} | 0, I)
\]
该分布通过参数 \(\theta\) 进行变换。然而，直接计算该参数是困难的，因此通常采用**神经网络**进行近似。

对于每个细胞，生成分布可以表示为以下积分：
\[
p(x_n | \theta) = \int p(z_n | \theta) p(x_n | z_n, \theta) dz_n
\]
然而，由于后验分布 \( p(z_n | x_n, \theta) \) **难以直接计算**，研究者引入了一个变分分布 \( q(z_n | x_n, \phi) \) 作为近似。假设该变分分布服从一个多元高斯分布，其均值 \(\mu_{\phi}(x_n)\) 和标准差 \(\sigma_{\phi}(x_n)\) 均由神经网络建模。

该模型的优化目标是让**相似的细胞在潜在空间中具有相似的后验分布**，因此低维潜在空间能够**有效保留高维数据的距离关系**，从而实现高效的降维。

---

### **总结**
这段话主要讨论了 **维度灾难及其解决方案**，重点介绍了**传统降维方法**（PCA、t-SNE、UMAP）和**基于深度学习的降维方法**（如 Scvis）。

1. **维度灾难的影响**：
   - 维度增加导致数据稀疏，使得数据点之间的相似性降低。
   - 需要降维方法来提高数据分析的有效性。

2. **降维方法分类**：
   - **参数化方法（Parametric Methods）**：
     - 通过显式数学模型学习数据表示（如 PCA、scPCA、FastRNA）。
   - **非参数化方法（Non-Parametric Methods）**：
     - 通过概率或度量学习进行降维（如 t-SNE、UMAP）。

3. **深度学习在降维中的应用**：
   - 深度学习（DL）是一种强大的参数化方法，可自动学习特征表示。
   - **Scvis** 基于 VAE 框架，结合生成模型和变分推断，能够进行高维到低维的双向映射。
   - 通过 **变分自编码器（VAE）**，Scvis 使得相似细胞在低维空间中的分布保持一致，从而保留高维数据的结构。

4. **数学原理**：
   - 通过神经网络近似高维数据的后验分布。
   - 目标是最小化高维数据和低维嵌入之间的损失，以保留数据的全局和局部结构。

**最终，Scvis 作为一种深度学习驱动的降维方法，能够高效地处理单细胞 RNA 测序数据，使得数据点在低维空间中保持原始的生物学关系，同时提高计算效率和可解释性。**
## 名词解释：

- **数据稀疏性 (Data Sparsity)**
指在单细胞和空间组学数据中，由于技术噪声、低表达信号、捕获效率不足等原因，数据矩阵中大部分值为零或缺失的现象。这种稀疏性常常增加下游分析（如聚类、降维、差异表达检测）的难度。

- **多样性 (Diversity)**
指在单细胞和空间组学中观察到的细胞、基因表达或空间结构的异质性。多样性反映了生物样本中不同细胞类型、状态及其在组织中分布的复杂性，是理解生物体功能和发育状态的重要指标。

- **数据稀缺性 (Scarcity)**
指在某些特定情况下，单细胞或空间组学数据中关于某些细胞类型、低丰度分子或特定区域的样本数量不足的问题。数据稀缺性会导致统计分析和模型构建时样本不足，进而影响研究结果的可靠性和泛化能力。

- **相关性 (Correlation)**
指单细胞和空间组学数据中不同变量（如基因表达水平、细胞间相互作用、空间位置等）之间的依赖关系。通过研究这些相关性，可以揭示细胞之间的功能联系、基因调控网络以及空间组织结构对生物学功能的影响。

- **多模态集成 (Multimodal Integration)**
指在同一研究中，利用不同类型的测量数据（如转录组、蛋白质组、表观基因组、空间信息等），通过数据融合和联合分析，揭示不同数据模式之间的相互关系和协同作用，从而提供更全面的生物学洞察。

- **多源集成 (Multi-source Integration)**
指整合来自不同来源、平台或实验条件的数据（可能包括多个样本、批次、实验室或技术平台），通过标准化和对齐方法消除系统性偏差，进而综合利用各数据源的互补信息，以提高分析结果的鲁棒性和可靠性。

- **多层感知机 (Multilayer Perceptron, MLP)**  
   指一种基础的前馈神经网络，由输入层、一个或多个隐藏层以及输出层组成，各层神经元之间采用全连接方式传递信息。实际上，MLP通常特指全连接神经网络，即每个神经元与下一层中所有神经元均有连接，但在某些变体中也可能会结合其他结构或激活函数来提升表现。

- **自编码器 (Autoencoder, AE)**  
   是一种无监督学习模型，旨在通过压缩（编码器）和重构（解码器）输入数据，从而提取数据的低维特征或潜在表示。AE在降噪、数据降维和特征提取等任务中具有广泛应用。

- **生成对抗网络 (Generative Adversarial Network, GAN)**  
   由生成器和判别器两部分组成的深度学习模型。生成器试图生成逼真的数据样本以“欺骗”判别器，而判别器则努力区分真实数据和生成数据。两者通过对抗性训练不断提升各自性能，从而实现高质量的数据生成。

- **卷积神经网络 (Convolutional Neural Network, CNN)**  
   利用卷积运算提取输入数据中的局部特征，常用于图像、视频、语音等具有空间或时间相关性的任务。CNN通过局部连接、共享权重和池化操作，有效降低参数数量并提高特征提取的鲁棒性。

- **图神经网络 (Graph Neural Network, GNN)**  
   针对图结构数据设计的神经网络模型，通过节点及其邻居信息的迭代融合，实现对图中节点、边或整体图的表征学习。GNN在社交网络、分子结构分析、知识图谱等领域有着广泛应用。

- **维数灾难（Curse of Dimensionality）** 指的是当数据的维度（特征数量）增加时，数据在空间中的分布会变得极其稀疏，从而给数据分析、建模和机器学习带来巨大挑战的现象。以下详细解释这一概念，并举例说明其影响：

    1. **空间体积的爆炸性增长**  
    在低维空间中，数据点往往能够较为密集地分布，因而距离度量（例如欧氏距离）可以有效地反映数据点之间的相似性。然而，随着维度的增加，空间的体积呈指数级增长。例如，在二维空间中，一个边长为1的正方形的面积为1；而在十维空间中，一个边长为1的超立方体的体积为1，但其边缘的“覆盖”比例远低于二维或三维情况。这意味着，如果我们想在高维空间中保持数据点的“密集”分布，就需要指数级增加数据量。

    2. **样本稀疏性与距离度量失效**  
    当数据点分布在高维空间中时，即使样本数量较多，也往往相对于整个空间而言显得非常稀疏。此时，传统的距离度量（如欧氏距离）在高维空间中可能变得无效，因为所有数据点之间的距离趋于相近，难以区分哪些点是真正“近”的，这使得基于距离的算法（如最近邻搜索、聚类等）的性能急剧下降。

    3. **计算复杂度增加**  
    数据维度增加不仅对存储和表示带来挑战，也使得计算复杂度显著上升。很多算法在高维空间中的时间和空间复杂度会急剧上升，导致计算成本增加，甚至在样本数量不大时也难以有效执行。

    举例说明

    假设你在二维平面上有一组数据点，你可以使用最近邻方法来查找与某个查询点距离最接近的数据点。在二维空间中，如果数据点分布较为均匀，大多数点之间的距离差异明显，那么最近邻方法能较好地找到最相似的数据点。

    然而，如果将问题扩展到高维空间，比如100维空间，由于空间体积急剧增大，数据点之间的相对距离变得十分接近，区分“近”和“远”的标准变得模糊。即使有大量数据，某个查询点与所有数据点的距离可能几乎相等，这使得最近邻方法难以找到真正有意义的近邻，从而导致模型性能下降。

    此外，考虑一个简单的例子：在二维空间中，要覆盖一个单位圆，你只需要较少的点就能较好地表示这个圆。然而，在100维空间中，一个“单位球”的体积相对于它的“边缘”会急剧减小，若不使用指数级增加的数据量，就无法充分覆盖整个空间，导致样本极为稀疏，从而影响任何基于样本密度的统计或机器学习方法。

    这种现象就是“维数灾难”，它提醒我们在设计高维数据分析或机器学习模型时，需要考虑如何降维、正则化或采用适合高维数据特点的方法，以避免数据稀疏和计算复杂度带来的问题。

- **荧光激活细胞分选（Fluorescence-Activated Cell Sorting, FACS）** 是一种基于流式细胞仪技术的细胞分选方法。其原理是利用荧光标记物对细胞进行染色，并在流式细胞仪中借助激光照射使细胞发出特定波长的荧光信号。通过对这些信号的检测与分析，仪器可以识别细胞的不同特性（如蛋白质表达、细胞状态等），并基于预设的参数将细胞分选到不同的容器中。该技术具有高通量、精确和实时分选的优势，广泛应用于免疫学、肿瘤学、干细胞研究等领域。

---

- **参数化方法 vs. 非参数化方法**
在机器学习和数据分析中，**参数化方法（Parametric Methods）** 和 **非参数化方法（Non-Parametric Methods）** 是两种不同的建模方式。它们的主要区别在于是否显式定义模型参数，并基于这些参数来进行预测或降维。


**1. 参数化方法（Parametric Methods）**
**定义**：
- **参数化方法假设数据服从某种固定的数学模型，并用有限个参数来描述该模型。**
- 在训练过程中，这些参数通过数据学习得到，一旦训练完成，模型的复杂度（参数数量）保持固定。

**特点**：
1. **固定数量的参数**：无论数据量如何变化，参数的数量不会改变。
2. **可解释性强**：通常有明确的数学表达式，便于理解（如 PCA）。
3. **计算高效**：由于参数数量有限，计算和存储成本较低。
4. **泛化性强**：适用于数据量有限但符合假设的情况。

**例子**：
- **PCA（主成分分析）**：
  - 假设数据可以通过正交变换投影到一个低维子空间，并通过**特征值分解（Eigen Decomposition）**或**奇异值分解（SVD）**求解。
  - 低维表示是原始数据的线性组合，参数是特征向量。
- **LDA（线性判别分析）**：
  - 通过计算类内方差和类间方差的比值来投影数据，找到最大区分度的方向。
- **神经网络（如 VAE, 深度学习）**：
  - 通过训练学习一组固定参数（权重和偏置），然后在推理时使用这些固定参数进行预测。


**2. 非参数化方法（Non-Parametric Methods）**
**定义**：
- **非参数方法不对数据的分布或变换形式做特定假设，而是通过数据自身的结构进行建模。**
- 这些方法通常依赖于局部数据关系，并不直接学习一个固定数量的参数。

**特点**：
1. **参数数量随数据量增长**：模型的复杂度取决于数据量，随着数据增多，模型也会变得更加复杂。
2. **灵活性强**：适用于复杂、非线性或未知分布的数据。
3. **计算复杂度较高**：因为需要存储和查询大量数据点，计算可能较慢。

**例子**：
- **k 近邻（KNN）分类**：
  - 直接存储所有训练数据，在预测时计算新数据与训练数据的距离，并选取最近的 \( k \) 个邻居进行投票。
- **t-SNE（t 分布随机邻域嵌入）**：
  - 通过计算数据点之间的概率相似性，将高维数据映射到低维，同时保持局部相似性。
  - 没有固定的转换矩阵，而是基于数据点对之间的关系动态计算映射。
- **UMAP（统一流形逼近与投影）**：
  - 通过构造高维数据的**邻接图（graph）**，然后通过优化过程将数据映射到低维空间，保留局部和全局结构。


**3. 为什么 UMAP 是非参数方法？**
**UMAP（Uniform Manifold Approximation and Projection）是一种非参数方法，原因如下：**

**1. UMAP 没有显式的固定参数映射**
- 传统的参数化方法（如 PCA）会学习一个固定的线性变换矩阵 \( W \)，然后使用该矩阵将数据投影到低维空间：
  \[
  X_{\text{low}} = W X_{\text{high}}
  \]
  - **PCA 的投影矩阵 \( W \) 是固定的，适用于所有新数据点。**

- **而 UMAP 不学习这样的固定映射**。UMAP 通过构造**邻接图**来表示数据点之间的关系，然后在低维空间中优化点的位置，使得相邻的数据点尽可能接近：
  - 它不提供一个直接的数学公式来从高维数据生成低维数据。
  - 每次运行 UMAP，可能得到不同的嵌入结果，因为优化过程具有随机性。

**2. UMAP 依赖数据点之间的关系，而非全局参数**
- UMAP 通过构造**KNN（k 近邻）图**来建立高维数据点之间的连接：
  1. 计算数据点的最近邻。
  2. 使用高斯核或其他方式计算局部相似性。
  3. 在低维空间中，通过梯度下降优化保持这些相邻点的相对位置。
- **由于 UMAP 仅存储数据点间的相似性，而不是一个固定的参数映射，因此属于非参数方法**。

**3. UMAP 不能直接用于新数据**
- **PCA 是一个参数化方法，因此可以用训练好的投影矩阵 \( W \) 直接对新数据进行降维**。
- **UMAP 需要重新计算 KNN 图并重新优化低维表示，无法直接对新数据进行映射**，除非进行额外的训练（如使用 UMAP 的 `transform()` 方法，但这仍然是通过数据关系计算得到的，而非固定参数）。


**4. 参数化方法 vs. 非参数方法对比**
| 特性 | 参数化方法（PCA, LDA, VAE） | 非参数化方法（t-SNE, UMAP, KNN） |
|------|--------------------------|--------------------------|
| **假设** | 有固定的数学模型，如线性变换 | 不做特定假设，依赖数据点关系 |
| **参数数量** | 固定（如 PCA 的特征向量个数） | 随数据量变化（如 UMAP 的邻接图） |
| **计算复杂度** | 计算较快，可直接投影新数据 | 计算较慢，每次都需要构造邻接关系 |
| **适用场景** | 适用于结构明确的数据，如正态分布 | 适用于复杂非线性数据，如流形结构 |
| **能否处理新数据** | 可以（如 PCA 直接使用已训练的映射） | 需要重新计算（如 UMAP 需要重新构造图） |

| **方法** | **参数化方法（PCA, 逻辑回归, GPT）** | **非参数化方法（KNN, t-SNE, UMAP）** |
|---------|--------------------------------|-------------------------------|
| **参数数量** | 固定 | 随数据变化 |
| **计算方式** | 学习全局映射 | 依赖局部关系 |
| **适用于新数据** | 直接推理 | 需要重新计算 |
| **例子** | GPT, BERT, ViT, CNN | KNN, UMAP, t-SNE |

**5. 结论**
- **参数化方法**：学习一个固定的数学模型（如 PCA 的投影矩阵），适用于数据分布较稳定的情况。
- **非参数化方法**：不对数据分布做特定假设，依赖数据点之间的关系（如 UMAP 的邻接图），适用于非线性结构的数据。

**UMAP 是非参数方法的主要原因**：
1. 它没有学习一个固定的转换矩阵，而是依赖数据点间的相似性进行降维。
2. 它通过 KNN 计算局部结构，而不是使用全局数学模型进行变换。
3. 它无法直接对新数据进行投影，而需要重新计算邻接关系。

这使得 UMAP 在高维数据降维时表现出色，特别适用于 **单细胞 RNA 测序（scRNA-seq）**、**图像嵌入** 等复杂数据集，但相较于 PCA，其计算成本更高。
--- 