# **文献解析与总结** **"A cell atlas foundation model for scalable search of similar human cells"**（一个用于可扩展搜索相似人类细胞的细胞图谱基础模型），发表于 *Nature* (Vol 638, 2025年2月27日)。

#### **1. 研究背景**
单细胞RNA测序（scRNA-seq）已经描绘了数亿个人类细胞的表达图谱，涵盖不同器官、疾病、发育阶段以及实验扰动条件。然而，如何在这些大规模数据集中高效搜索与输入细胞状态相似的细胞仍然是一个挑战。当前的问题包括：
- 数据集整合困难，缺乏统一的低维表示。
- 无法定义细胞相似性的标准度量。
- 缺乏高效的全细胞搜索方法。

为了应对这些问题，该研究开发了 **SCimilarity**，一个基于 **度量学习（Metric Learning）** 的深度学习框架，能够快速查询数千万个细胞，并在全身尺度上识别转录组相似的细胞状态。

---

#### **2. 研究内容**
SCimilarity 是一个 **深度度量学习（Deep Metric Learning）** 模型，能够学习 **单细胞表达数据的低维表示**，以便快速搜索相似细胞。它的核心方法包括：
- **Triplet Loss（三元组损失）**：优化嵌入空间，使得同类型细胞在低维空间内更加接近，而不同类型细胞则更远。
- **均方误差（MSE）损失**：保证嵌入空间能够保留细胞表达的原始信息。
- **训练数据**：该模型在 23.4 百万个细胞的图谱上进行训练，覆盖 412 个不同的 scRNA-seq 研究数据。

该研究使用 SCimilarity 进行以下分析：
1. **查询间质性肺病（ILD）中的巨噬细胞和成纤维细胞**，并找到它们在其他纤维化疾病和组织中的类似细胞状态。
2. **搜索体外培养模型**，发现 3D 水凝胶系统能重现特定的细胞状态，并通过实验验证了该体外系统能模拟纤维化相关的巨噬细胞状态。
3. **模型泛化能力测试**，显示 SCimilarity 能有效地集成不同测序技术（如 10x Genomics、SMART-Seq2 等）生成的数据，并能够在新的数据集中进行查询而无需重新训练。

---

#### **3. 文章的创新点**
该研究的创新性主要体现在以下几个方面：
1. **首个基于度量学习（Metric Learning）的细胞相似性查询框架**：
   - 以 Triplet Loss 和 MSE 结合的方式进行训练。
   - 使得不同研究、不同测序技术的细胞数据能够映射到相同的低维空间中，便于查询和比对。
  
2. **跨研究、跨组织、跨疾病的可扩展细胞查询方法**：
   - 传统方法（如 PCA、autoencoders）主要用于单个数据集，而 SCimilarity 能够无缝查询超过 23.4 百万个细胞，并在整个人体尺度上搜索类似细胞状态。

3. **高效的计算性能**：
   - SCimilarity 能在 **0.05 秒内完成对 1000 万个细胞的搜索**，比基于基因签名的传统方法快 **多个数量级**。

4. **发现新的体外模型并进行实验验证**：
   - 该研究使用 SCimilarity 发现 **3D 水凝胶培养系统** 能够重现体内的巨噬细胞状态，并通过实验验证其生物学意义。

5. **模型具备良好的泛化性**：
   - SCimilarity 训练于 10x Genomics 数据，但在不同测序平台（如 SMART-Seq2、Drop-seq）上的表现依然稳健，能够广泛适用于不同数据集。

---

#### **4. 什么是 Metric Learning（度量学习）？**
**度量学习（Metric Learning）** 是一种 **监督学习方法**，其目标是学习一个合适的 **距离度量**，使得：
- **相似的样本在低维空间中更接近**（例如，同一细胞类型的样本）。
- **不相似的样本彼此远离**（例如，不同细胞类型的样本）。

SCimilarity 采用 **三元组损失（Triplet Loss）** 进行训练，核心思想是：
1. 选择一个 **锚点样本（Anchor）**，如某个细胞的基因表达谱。
2. 选择一个 **正样本（Positive）**，即与 Anchor 类似的细胞（相同细胞类型）。
3. 选择一个 **负样本（Negative）**，即与 Anchor 不相似的细胞（不同细胞类型）。
4. 通过训练，使得 **正样本和锚点的距离最小，而负样本和锚点的距离最大**。

这种方法常用于 **图像识别（如人脸识别）**，在生物信息学中首次被用于 **大规模单细胞数据的相似性搜索**。

---

### **总结**
本文提出了一种新的 **基于度量学习的单细胞查询方法（SCimilarity）**，实现了：
- 高效查询细胞相似性，能够快速搜索数千万个细胞。
- 结合 Triplet Loss 和 MSE 进行优化，实现对细胞状态的精确表征。
- 发现新的体外模型，并通过实验验证其生物学相关性。
- 泛化能力强，适用于不同的测序平台和研究背景。

**SCimilarity 为单细胞数据分析提供了一种强大的工具，能够帮助研究者高效挖掘细胞状态的相似性，并推动生物医学研究的发展。**

### **度量学习（Metric Learning）的历史、算法原理与主要应用**

---

## **1. 度量学习的历史**
度量学习（Metric Learning）是一种监督学习方法，专注于学习适用于特定任务的 **距离度量（Distance Metric）**，以便在给定的数据空间中优化相似性计算。

### **1.1 早期研究**
- **Mahalanobis 距离 (1936)**：度量学习的基础概念之一。Mahalanobis 距离考虑了数据的协方差，使得距离计算能够适应数据分布。
- **Fisher 判别分析 (Fisher Discriminant Analysis, 1936)**：通过投影数据到一个低维子空间，以优化类别可分性。

### **1.2 20 世纪 90 年代：经典度量学习**
- **Large Margin Nearest Neighbor (LMNN, 2005)**：
  - 通过优化 **k 近邻（KNN）分类器**的性能，学习一个变换矩阵，使得同类别样本更接近，不同类别的样本更远。
- **Neighbourhood Components Analysis (NCA, 2004)**：
  - 通过优化最近邻分类器的预测概率，学习一个线性变换，使得最近邻分类误差最小化。
- **Information-Theoretic Metric Learning (ITML, 2007)**：
  - 通过最小化信息熵的方式来优化距离度量，使得数据点更符合先验知识。

### **1.3 深度度量学习的兴起**
- **FaceNet (2015, Google)**：
  - 使用 **三元组损失（Triplet Loss）** 学习人脸识别的嵌入向量，使得同一人的照片在低维空间中更接近，而不同人的照片则分布更远。
- **Siamese Network (1993, 2015 重新流行)**：
  - 通过 **孪生网络（Siamese Network）** 结构进行对比学习，使得相似样本距离最小，不相似样本距离最大。
- **Contrastive Learning (SimCLR, MoCo, 2020)**：
  - 通过 **对比损失（Contrastive Loss）** 进行无监督学习，使得相似样本在嵌入空间中更接近。

---

## **2. 度量学习的算法原理**
度量学习的核心目标是 **学习一个适合特定任务的距离度量**，使得相似的样本更接近，不相似的样本更远。关键的数学工具包括：

### **2.1 经典度量学习**
#### **(1) Mahalanobis 距离**
Mahalanobis 距离是 **最早的度量学习方法**，定义如下：
\[
D_M(x_i, x_j) = \sqrt{(x_i - x_j)^T M (x_i - x_j)}
\]
其中，\( M \) 是一个可学习的正定矩阵，表示一个变换。

#### **(2) Large Margin Nearest Neighbor (LMNN)**
- 目标：优化最近邻分类器，使得同类别点的距离更小，而不同类别的点距离更大。
- 约束：
  1. 目标点 \( x_i \) 应该尽可能靠近其同类别的 \( k \) 个最近邻。
  2. 目标点 \( x_i \) 应该远离不同类别的点。

目标函数：
\[
\sum_{i,j} \| L x_i - L x_j \|^2 + \lambda \sum_{i,j,l} \max(0, 1 + d_{ij} - d_{il})
\]
其中，\( L \) 是一个可学习的线性变换，\( d_{ij} \) 表示两个样本之间的欧氏距离。

---

### **2.2 深度度量学习**
深度度量学习使用神经网络学习非线性映射，使得相似的样本在低维嵌入空间中更加接近。主要方法包括：

#### **(1) Siamese Network（孪生网络）**
- 结构：使用 **共享权重的神经网络** 处理两个输入样本。
- 损失函数：
  - **对比损失（Contrastive Loss）**：
    \[
    L = (1 - Y) \frac{1}{2} D^2 + Y \frac{1}{2} \max(0, m - D)^2
    \]
    - \( Y = 1 \) 表示样本相似（距离应当较小）。
    - \( Y = 0 \) 表示样本不相似（距离应当较大）。

#### **(2) Triplet Loss（三元组损失）**
- 目标：优化 **三元组（Anchor, Positive, Negative）** 之间的关系，使得：
  \[
  D(A, P) + \alpha < D(A, N)
  \]
  其中：
  - \( A \)（Anchor）：基准样本。
  - \( P \)（Positive）：与 Anchor 相同类别的样本。
  - \( N \)（Negative）：与 Anchor 不同类别的样本。

损失函数：
\[
L = \sum_{i} \max(0, D(A_i, P_i) - D(A_i, N_i) + \alpha)
\]
其中，\( \alpha \) 是 **margin**（边界值），用于控制最近邻的距离差异。

#### **(3) Contrastive Learning（对比学习）**
- 通过对比不同样本的相似度来进行 **无监督学习**，主要方法包括：
  - **SimCLR (2020)**: 通过数据增强生成不同视角的样本，优化对比损失。
  - **MoCo (2020)**: 采用动量更新机制，增强表征学习。

---

## **3. 度量学习的主要应用**
由于度量学习能够学习最优的相似度度量，因此它广泛应用于 **计算机视觉、自然语言处理、生物信息学等领域**。

### **3.1 计算机视觉**
1. **人脸识别（Face Recognition）**
   - 典型方法：**FaceNet、DeepID、ArcFace**
   - 通过 Triplet Loss 学习人脸嵌入，使得相同个体的照片接近，不同个体的照片远离。

2. **目标跟踪（Object Tracking）**
   - 典型方法：**Siamese Network**
   - 通过度量学习判断当前帧与目标物体的相似性。

3. **图像检索（Image Retrieval）**
   - 通过度量学习优化 **嵌入空间**，实现高效的相似图像搜索。

---

### **3.2 自然语言处理（NLP）**
1. **文本匹配（Text Matching）**
   - 通过 **Siamese Network** 计算两个文本的相似度，用于 **问答系统、信息检索**。
2. **语义搜索（Semantic Search）**
   - 典型方法：**BERT + Metric Learning**
   - 通过度量学习优化语义嵌入，使得语义相似的文本靠近。

---

### **3.3 生物信息学**
1. **基因数据分析**
   - 通过度量学习优化基因表达数据的嵌入，使得相似的细胞更容易聚类。

2. **单细胞 RNA-seq 细胞类型预测**
   - 典型方法：**SCimilarity**
   - 通过 **Triplet Loss** 训练单细胞数据，使得相似的细胞在嵌入空间更接近。

---

## **4. 结论**
度量学习是一种 **强大的监督学习方法**，能够学习最优的距离度量，广泛应用于 **计算机视觉、NLP、生物信息学等领域**。从 **经典度量学习（Mahalanobis 距离）** 到 **深度度量学习（Siamese Network, Triplet Loss）**，该领域不断发展，并成为 **大规模数据分析和搜索的核心技术之一**。

## **度量学习（Metric Learning）整体算法过程**
度量学习的核心目标是 **学习一个合适的距离度量，使得相似的样本在低维空间内靠近，而不相似的样本远离**。常见的方法包括：
- **Mahalanobis 距离学习**（线性方法）
- **Siamese Network（孪生网络）**
- **Triplet Loss（三元组损失）**

我们通过 **Triplet Loss（三元组损失）** 来详细解析度量学习的算法过程，并通过一个简单的例子来帮助理解。

---

## **算法过程**
### **输入**
- 训练数据集 \( X = \{x_1, x_2, ..., x_n\} \)
- 每个样本 \( x_i \) 具有对应的类别标签 \( y_i \)

### **目标**
学习一个 **映射函数 \( f(x) \)**，将数据投影到低维嵌入空间，使得：
\[
\text{距离}(f(A), f(P)) + \alpha < \text{距离}(f(A), f(N))
\]
其中：
- **\( A \)**（Anchor）：基准样本
- **\( P \)**（Positive）：与 Anchor 相同类别的样本
- **\( N \)**（Negative）：与 Anchor 不同类别的样本
- **\( \alpha \)**：一个 margin（边界），确保相似样本更加接近

---

## **具体步骤**
### **步骤 1: 选择三元组（Triplet Selection）**
在训练数据集中，我们需要选择训练样本：
- **Anchor（A）**：一个随机选取的样本
- **Positive（P）**：与 A **相同类别** 的样本
- **Negative（N）**：与 A **不同类别** 的样本

#### **示例**
假设我们有 3 类数据：
- **类别 A**：猫 🐱
- **类别 B**：狗 🐶
- **类别 C**：马 🐴

对于一个 **Anchor（猫 🐱）**，我们可以选择：
- **Positive（🐱）**：另一张猫的照片
- **Negative（🐶）**：一张狗的照片

我们构造的三元组可能是：
\[
(A, P, N) = (\text{猫 🐱}, \text{猫 🐱}, \text{狗 🐶})
\]

**为什么要这样做？**
- 这样可以训练模型，使得 **相同类别的样本靠近**，不同类别的样本远离。
- 直接使用原始数据可能会导致模型学习不到类别的差异。

---

### **步骤 2: 计算样本嵌入**
使用一个神经网络 **\( f(x) \)**（例如 CNN）将输入数据映射到 **低维嵌入空间**。

#### **示例**
假设模型学习到：
- **猫 🐱 映射到** \( f(A) = (0.2, 0.5) \)
- **猫 🐱（Positive）映射到** \( f(P) = (0.3, 0.4) \)
- **狗 🐶（Negative）映射到** \( f(N) = (0.8, 0.9) \)

现在，我们在 **低维空间中计算欧式距离**：
\[
D(A, P) = \| f(A) - f(P) \|^2 = (0.2 - 0.3)^2 + (0.5 - 0.4)^2 = 0.02
\]
\[
D(A, N) = \| f(A) - f(N) \|^2 = (0.2 - 0.8)^2 + (0.5 - 0.9)^2 = 0.52
\]

---

### **步骤 3: 计算损失函数**
使用 **Triplet Loss（三元组损失）** 来优化模型：
\[
L = \max(0, D(A, P) - D(A, N) + \alpha)
\]
其中：
- 如果 **D(A, P) + α < D(A, N)**，说明模型已经学会了好的距离度量，损失为 0。
- 否则，损失会被优化，使得 **相似样本更接近，不相似样本更远**。

#### **示例**
假设我们设定 \( \alpha = 0.2 \)，计算损失：
\[
L = \max(0, 0.02 - 0.52 + 0.2) = \max(0, -0.3) = 0
\]
由于 **损失为 0**，说明当前的嵌入已经符合要求。

如果有一个 **新的负样本 🐴（马）**：
- **\( f(N) = (0.3, 0.5) \)**（和猫特别接近）
- 计算距离：
  \[
  D(A, N) = \| f(A) - f(N) \|^2 = (0.2 - 0.3)^2 + (0.5 - 0.5)^2 = 0.01
  \]
- 计算损失：
  \[
  L = \max(0, 0.02 - 0.01 + 0.2) = 0.21
  \]
  由于损失 > 0，模型需要更新，使得猫和马的距离增大。

---

### **步骤 4: 反向传播更新模型**
通过梯度下降 **更新模型参数**，使得：
1. \( f(A) \) 和 \( f(P) \) **距离更近**（相似样本靠近）。
2. \( f(A) \) 和 \( f(N) \) **距离更远**（不同类别分离）。

---

## **完整流程总结**
1. **选择三元组 (A, P, N)**
   - 选取 **同类别的 Positive** 和 **不同类别的 Negative**。
2. **计算嵌入 (f(A), f(P), f(N))**
   - 使用深度学习模型将数据映射到低维空间。
3. **计算 Triplet Loss**
   - 计算 \( D(A, P) \) 和 \( D(A, N) \) 之间的差值，并应用 margin 约束。
4. **反向传播更新模型**
   - 通过优化网络参数，使得：
     - \( A \) 和 \( P \) 更接近（相似样本聚类）。
     - \( A \) 和 \( N \) 更远（不同类别分离）。
5. **重复训练，直到收敛**
   - 训练多个 epoch，直到嵌入空间收敛。

---

## **为什么这样做？（算法原理解释）**
1. **三元组选择**：
   - 确保模型能够有效区分 **相似与不相似样本**，减少类别混淆。
2. **映射到低维嵌入空间**：
   - 使得数据点的表示更加紧凑，适用于高效搜索和分类。
3. **Triplet Loss 约束**：
   - 强制模型在优化过程中学习到有意义的类别间隔。
4. **迭代优化**：
   - 通过梯度下降不断更新，使得模型能够在新的数据上泛化。

---

## **应用**
### **1. 人脸识别（FaceNet）**
- 通过 Triplet Loss 训练人脸嵌入，使得同一个人的照片靠近，不同人的照片远离。

### **2. 图像检索（Image Retrieval）**
- 通过度量学习优化图像搜索，使得相似图像排名更靠前。

### **3. 细胞分类（SCimilarity）**
- 在单细胞 RNA-seq 数据中，通过度量学习寻找 **相似细胞类型**。

---

## **结论**
度量学习的核心是 **学习一个适用于特定任务的相似性度量**，通过三元组损失（Triplet Loss），我们可以 **在嵌入空间中优化数据分布，使得相似样本聚类，不相似样本分离**。该方法广泛应用于 **人脸识别、图像检索、生物信息学等领域**，是目前高效查询与分类的关键技术之一。